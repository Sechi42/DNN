{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "720f39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e185bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x =  torch.tensor([1, 2, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cae40c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afc616c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfc274b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "x[0] += 10\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "607b7f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c427c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "x0_val = x[0].item()\n",
    "print(x0_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382690ec",
   "metadata": {},
   "source": [
    "Ejemplo de tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65d52b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.ones(2,3)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90fd7c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9429, 0.2832, 0.0427],\n",
      "        [0.0490, 0.0010, 0.6269]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(2, 3)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b70d521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.empty(2, 3)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ae515",
   "metadata": {},
   "source": [
    "### Que es un tensor?\n",
    "\n",
    "Un tensor es una estructura de datos que generaliza los conceptos de escalares, vectores y matrices a dimensiones superiores. En términos simples:\n",
    "- Un escalar es un tensor de orden 0 (un solo número).\n",
    "- Un vector es un tensor de orden 1 (una lista de números).\n",
    "- Una matriz es un tensor de orden 2 (una tabla de números con filas y columnas).\n",
    "\n",
    "Un tensor puede tener más de dos dimensiones, lo que permite representar datos más complejos, como imágenes (3D), videos (4D) o cualquier otro tipo de datos multidimensionales. Los tensores son fundamentales en el aprendizaje profundo y se utilizan para almacenar y manipular datos en redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0d6480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x0000027F3DB0C820>\n",
      "tensor(-5.3000)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(-5.3)\n",
    "print(a.size)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1ace6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,3)\n",
    "print(a.size())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433f8b8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "La función `torch.ones(3, 4, 5)` crea un tensor de tres dimensiones (3D) donde:\n",
    "\n",
    "- El primer número (`3`) indica que hay 3 bloques o matrices.\n",
    "- El segundo número (`4`) indica que cada bloque tiene 4 filas.\n",
    "- El tercer número (`5`) indica que cada fila tiene 5 columnas.\n",
    "\n",
    "En términos generales, un tensor de forma `(3, 4, 5)` puede visualizarse como una colección de 3 matrices, cada una de tamaño 4x5. Más allá de matrices, los tensores permiten representar datos multidimensionales, como imágenes (por ejemplo, lotes de imágenes RGB pueden tener forma `(batch_size, canales, alto, ancho)`).\n",
    "\n",
    "**Resumen:**  \n",
    "- Escalar: tensor de 0 dimensiones (un solo número).\n",
    "- Vector: tensor de 1 dimensión (una lista).\n",
    "- Matriz: tensor de 2 dimensiones (tabla de números).\n",
    "- Tensor 3D: colección de matrices (por ejemplo, `torch.ones(3, 4, 5)`).\n",
    "\n",
    "Esto es útil en aprendizaje profundo para organizar datos complejos y trabajar con lotes de ejemplos, canales de color, secuencias, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3edf084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3,4,5)\n",
    "print(a.size())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3be8d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n",
      "tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "# 2 Grupos de 3 matrices de 4x5\n",
    "a = torch.ones(2,3,4,5)\n",
    "print(a.size())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0fdcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5, 2])\n",
      "tensor([[[[[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]],\n",
      "\n",
      "          [[1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.],\n",
      "           [1., 1.]]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,3,4,5,2)\n",
    "print(a.size())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec10cb",
   "metadata": {},
   "source": [
    "### Operaciones aritméticas básicas con tensores en PyTorch\n",
    "\n",
    "A continuación se muestran ejemplos de suma, resta, multiplicación, división y operaciones elemento a elemento entre tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2e8b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma: tensor([5., 7., 9.])\n",
      "Resta: tensor([-3., -3., -3.])\n",
      "Producto: tensor([ 4., 10., 18.])\n",
      "División: tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# Suma, resta, multiplicación y división elemento a elemento\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "suma = x + y\n",
    "resta = x - y\n",
    "producto = x * y\n",
    "division = x / y\n",
    "\n",
    "print('Suma:', suma)\n",
    "print('Resta:', resta)\n",
    "print('Producto:', producto)\n",
    "print('División:', division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b82a794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potencia: tensor([1., 4., 9.])\n",
      "Raíz cuadrada: tensor([1.0000, 1.4142, 1.7321])\n",
      "Suma total: tensor(6.)\n",
      "Media: tensor(2.)\n",
      " tensor([1., 4., 9.])\n",
      "Raíz cuadrada: tensor([1.0000, 1.4142, 1.7321])\n",
      "Suma total: tensor(6.)\n",
      "Media: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# Otras operaciones: potencia, raíz cuadrada, suma total, media\n",
    "potencia = x ** 2\n",
    "raiz = torch.sqrt(x)\n",
    "suma_total = torch.sum(x)\n",
    "media = torch.mean(x)\n",
    "\n",
    "print('Potencia:', potencia)\n",
    "print('Raíz cuadrada:', raiz)\n",
    "print('Suma total:', suma_total)\n",
    "print('Media:', media)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd9b7e",
   "metadata": {},
   "source": [
    "Good an bad Numpy copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55e0dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3452, 0.7910, 0.0868],\n",
      "        [0.5146, 0.6752, 0.3357]]) \n",
      "\n",
      "[[0.34523594 0.7910432  0.0867933 ]\n",
      " [0.51459706 0.6751846  0.33565342]]\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([[0.6905, 1.5821, 0.1736],\n",
      "        [1.0292, 1.3504, 0.6713]]) \n",
      "\n",
      "[[0.6904719  1.5820864  0.1735866 ]\n",
      " [1.0291941  1.3503692  0.67130685]]\n"
     ]
    }
   ],
   "source": [
    "# Bad copy: torch -> numpy\n",
    "x = torch.rand(2, 3)\n",
    "y = x.numpy()\n",
    "\n",
    "print(x, '\\n')\n",
    "print(y)\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "x.mul_(2)\n",
    "print(x, '\\n')\n",
    "print(y)  # y también cambia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81a6d525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4330, 0.3332, 0.9374],\n",
      "        [0.3491, 0.9291, 0.6169]]) \n",
      "\n",
      "[[0.43304473 0.33323538 0.9373702 ]\n",
      " [0.3490641  0.9290798  0.61685437]]\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([[0.8661, 0.6665, 1.8747],\n",
      "        [0.6981, 1.8582, 1.2337]]) \n",
      "\n",
      "[[0.43304473 0.33323538 0.9373702 ]\n",
      " [0.3490641  0.9290798  0.61685437]]\n"
     ]
    }
   ],
   "source": [
    "# Good copy: torch -> numpy\n",
    "x = torch.rand(2, 3)\n",
    "y = x.clone().numpy()\n",
    "# or y = x.numpy() + 0\n",
    "\n",
    "print(x, '\\n')\n",
    "print(y)\n",
    "\n",
    "print('-'*80)\n",
    "x.mul_(2)\n",
    "print(x, '\\n')\n",
    "print(y)  # y no cambia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16284e6",
   "metadata": {},
   "source": [
    "Copias en Numpy: buenas y malas prácticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb34213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17582504 0.80838424 0.44909747 0.35425312]\n",
      " [0.55197239 0.14620674 0.90574164 0.61984024]\n",
      " [0.12961403 0.81825249 0.45198044 0.09402041]] \n",
      "\n",
      "tensor([[0.1758, 0.8084, 0.4491, 0.3543],\n",
      "        [0.5520, 0.1462, 0.9057, 0.6198],\n",
      "        [0.1296, 0.8183, 0.4520, 0.0940]], dtype=torch.float64)\n",
      "--------------------------------------------------------------------------------\n",
      "[[-0.82417496 -0.19161576 -0.55090253 -0.64574688]\n",
      " [-0.44802761 -0.85379326 -0.09425836 -0.38015976]\n",
      " [-0.87038597 -0.18174751 -0.54801956 -0.90597959]] \n",
      "\n",
      "tensor([[-0.8242, -0.1916, -0.5509, -0.6457],\n",
      "        [-0.4480, -0.8538, -0.0943, -0.3802],\n",
      "        [-0.8704, -0.1817, -0.5480, -0.9060]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Bad copy: numpy -> torch\n",
    "z = np.random.random((3, 4))\n",
    "zt = torch.from_numpy(z)\n",
    "print(z, '\\n')\n",
    "print(zt)\n",
    "print('-'*80)\n",
    "\n",
    "zt.sub_(1)\n",
    "# z también cambia\n",
    "print(z, '\\n')\n",
    "print(zt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c242fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14816456 0.465921   0.05720473 0.39231696]\n",
      " [0.6331888  0.39644009 0.66707104 0.76954236]\n",
      " [0.10383818 0.30408229 0.20738019 0.21212   ]] \n",
      "\n",
      "tensor([[0.1482, 0.4659, 0.0572, 0.3923],\n",
      "        [0.6332, 0.3964, 0.6671, 0.7695],\n",
      "        [0.1038, 0.3041, 0.2074, 0.2121]], dtype=torch.float64)\n",
      "--------------------------------------------------------------------------------\n",
      "[[0.14816456 0.465921   0.05720473 0.39231696]\n",
      " [0.6331888  0.39644009 0.66707104 0.76954236]\n",
      " [0.10383818 0.30408229 0.20738019 0.21212   ]] \n",
      "\n",
      "tensor([[-0.8518, -0.5341, -0.9428, -0.6077],\n",
      "        [-0.3668, -0.6036, -0.3329, -0.2305],\n",
      "        [-0.8962, -0.6959, -0.7926, -0.7879]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Good copy: numpy -> torch\n",
    "z = np.random.random((3, 4))\n",
    "zt = torch.tensor(z)\n",
    "print(z, '\\n')\n",
    "print(zt)\n",
    "print('-'*80)\n",
    "\n",
    "zt.sub_(1)\n",
    "# z no cambia\n",
    "print(z, '\\n')\n",
    "print(zt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b08d26",
   "metadata": {},
   "source": [
    "GPU y CPU en PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2407c976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "tensor([[[2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.]]])\n",
      "tensor([[[2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.],\n",
      "         [2., 2., 2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Yes')\n",
    "    dev = torch.device(\"cuda\")\n",
    "    \n",
    "    x = torch.ones(2, 3, 4, device=dev)  # directly create a tensor on GPU\n",
    "    y = torch.ones(2, 3, 4)\n",
    "    y = y.to(dev)  # or .to(\"cuda\")\n",
    "    \n",
    "    z = x + y\n",
    "    z = z.to(\"cpu\")  # or .to(dev)\n",
    "    print(z)\n",
    "    \n",
    "else:\n",
    "    print('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743a1cc",
   "metadata": {},
   "source": [
    "## Diferenciación\n",
    "\n",
    "El problema es, que dada una función $f : \\mathbb{R} \\rightarrow \\mathbb{R}$, computa su derivada con respecto a $x_i$:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "Hence, tenemos que computar el gradiente * evaluado en un punto en particular:\n",
    "$$\n",
    "\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)\n",
    "$$\n",
    "\n",
    "#### Diferenciación simbólica\n",
    "\n",
    "Supongamos $f(x) = sin(2x)$. Computar la derivada es sencillo:\n",
    "$$\n",
    "\\frac{df}{dx} = 2cos(2x)\n",
    "$$\n",
    "\n",
    "en el punto 3.5 es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87172afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*cos(2*x)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.50780450868661$"
      ],
      "text/plain": [
       "1.50780450868661"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x = sym.symbols('x')\n",
    "\n",
    "df = sym.diff(sym.sin(2 * x), x)\n",
    "print(df)\n",
    "df.subs(x, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de488a9b",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "- Diferenciación exacta\n",
    "- Una vez que la derivada está computada, evaluarla en cualquier punto es rápido.\n",
    "\n",
    "Contras:\n",
    "- Computacionalmente costoso para funciones complejas.\n",
    "- Para aplicaciones reales, diferenciacion simbólica no es práctica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bbdd6",
   "metadata": {},
   "source": [
    "## Diferenciación numérica\n",
    "\n",
    "Dada una función $f : \\mathbb{R} \\rightarrow \\mathbb{R}$, la derivada en un punto $x$ se puede aproximar como:\n",
    "$$\n",
    "\\frac{df}{dx} \\approx \\frac{f(x + h * e_i) - f(x)}{h}\n",
    "$$\n",
    "para un valor pequeño de $h$.\n",
    "\n",
    "Donde $e_i$ es el elemento de la base canónica (un vector con 1 en la posición $i$ y 0 en las demás).\n",
    "Esto para diferenciacion finita.\n",
    "\n",
    "Imagina que $f(x) = sin(2x)$ y queremos la derivada en el punto 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86891ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "h = 1e-4\n",
    "x0 = 3.5\n",
    "n = 1\n",
    "\n",
    "ei = np.diag(np.ones(n))\n",
    "print(ei)\n",
    "\n",
    "def f1(x):\n",
    "    return np.sin(2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb876e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5076731013186073\n"
     ]
    }
   ],
   "source": [
    "val = (f1(x0 + h * ei[0]) - f1(x0)) / h\n",
    "print(val.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e572a82",
   "metadata": {},
   "source": [
    "Ahora con $f(x) = 3x_1x_3-x_2^2+5x_3^2$ computa la derivada para el punto (1, 2, 3):\n",
    "\n",
    "$$\n",
    "\\nabla f(1, 2, 3) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e8052a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1.0, 2.0, 3.0])\n",
    "n = 3\n",
    "ei = np.diag(np.ones(n))\n",
    "print(ei)\n",
    "\n",
    "def f2(x):\n",
    "    return 3 * x[0] * x[2] -2*x[1]**2 + 5 * x[2]**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ce7d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1299997.          -920008.0002        80144.00450005]\n"
     ]
    }
   ],
   "source": [
    "val = (f2(x0 + h * ei) - f2(x0)) / h\n",
    "print(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1570994",
   "metadata": {},
   "source": [
    "El erro de paximación of forward finite difference is $O(h)$. vamos a intentar usar central finite difference, que tiene un error de $O(h^2)$:\n",
    "$$\n",
    "\\frac{df}{dx} \\approx \\frac{f(x + h * e_i) - f(x - h * e_i)}{2h}\n",
    "$$\n",
    "par algun 0 < h << 1. donde $e_1$ Pertenece a la base canónica. y es el i elemento de la base canónica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a12ad24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.          -8.         144.00000005]\n"
     ]
    }
   ],
   "source": [
    "val = (f2(x0 + h * ei) - f2(x0 - h * ei)) / (2 * h)\n",
    "print(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612d580",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- En algunos escenarios, la presicion es buena.\n",
    "- Fácil de implementar.\n",
    "\n",
    "Contras:\n",
    "- La aproximación puede ser inexacta si h no es lo suficientemente pequeño.\n",
    "- Depende del comportamiento local de la función. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607af79b",
   "metadata": {},
   "source": [
    "## Diferenciación automática\n",
    "\n",
    "Supongamos que $f(x) = sin(2x)$ y computar la derivada en el punto 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f877a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5000, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x0 = torch.tensor(3.5, requires_grad=True)\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac0abb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6570, grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f1 = torch.sin(2 * x0)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3fe46d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5078)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = f1.backward()\n",
    "print(x0.grad)  # df/dx at x=3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39890c",
   "metadata": {},
   "source": [
    "Ahora con $f(x) = 3x_1x_3-x_2^2+5x_3^2$ computa la derivada para el punto (1, 2, 3):\n",
    "\n",
    "$$\n",
    "\\nabla f(1, 2, 3) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "099f95f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d0fc9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(136., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f2 = 3 * x0[0] * x0[2] - 2 * x0[1]**2 + 5 * x0[2]**3\n",
    "print(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d220d2ec",
   "metadata": {},
   "source": [
    "### Visualización del grafo computacional en PyTorch\n",
    "\n",
    "PyTorch permite inspeccionar el grafo de operaciones que se construye para calcular derivadas automáticas. El atributo `.grad_fn` de un tensor resultado muestra el nodo raíz del grafo. Podemos recorrer el grafo hacia atrás usando `.next_functions`.\n",
    "\n",
    "A continuación se muestra cómo visualizar el grafo para la función $f(x) = 3x_1x_3-x_2^2+5x_3^3$ evaluada en $x=[1,2,3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6a4e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn de f2: <AddBackward0 object at 0x0000027F3706CCA0>\n",
      "Paso 0: <AddBackward0 object at 0x0000027F3706CCA0>\n",
      "Paso 1: <SubBackward0 object at 0x0000027F369D92D0>\n",
      "Paso 2: <MulBackward0 object at 0x0000027F3706CCA0>\n",
      "Paso 3: <MulBackward0 object at 0x0000027F369D92D0>\n",
      "Paso 4: <SelectBackward0 object at 0x0000027F3706CCA0>\n"
     ]
    }
   ],
   "source": [
    "# Inspección básica del grafo computacional\n",
    "print('grad_fn de f2:', f2.grad_fn)\n",
    "\n",
    "# Recorrer el grafo hacia atrás\n",
    "current = f2.grad_fn\n",
    "for i in range(5):\n",
    "    print(f'Paso {i}:', current)\n",
    "    if hasattr(current, 'next_functions') and current.next_functions:\n",
    "        current = current.next_functions[0][0]\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  9.,  -8., 138.])\n"
     ]
    }
   ],
   "source": [
    "f2.backward()\n",
    "print(x0.grad)  # df/dx at x=[1,2,3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33f186",
   "metadata": {},
   "source": [
    "## Simple ejemplo\n",
    "\n",
    "Supon "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
