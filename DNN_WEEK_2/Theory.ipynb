{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf890da",
   "metadata": {},
   "source": [
    "## Regresión Polinómica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53377eb",
   "metadata": {},
   "source": [
    "Modelo de acuerdo al orden\n",
    "\n",
    "Orden 0\n",
    "$$\n",
    "h^{(0)}(x) = w_0\n",
    "$$\n",
    "\n",
    "Orden 1:\n",
    "$$\n",
    "h^{(1)}(x) = w_1 x_1 + w_2 x_2 + ... + w_n x_n + w_0\n",
    "$$\n",
    "\n",
    "Orden 2:\n",
    "$$\n",
    "h^{(2)}(x) = w_1 x_1^2 + w_2 x_2^2 + ... + w_n x_n^2 + w_{1,2} x_1 x_2 + w_{1,3} x_1 x_3 + ... + w_{n-1,n} x_{n-1} x_n + w_0\n",
    "$$\n",
    "\n",
    "Para encontrar los parámetros óptimos $\\theta$ que minimizan la función de costo $J(\\theta)$, se utiliza la notación de $\\arg\\min$:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta} J(\\theta)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\theta^*$ es el valor óptimo de los parámetros.\n",
    "- $J(\\theta)$ es la función de costo (por ejemplo, el error cuadrático medio).\n",
    "\n",
    "La igualdad se cumple cuando:\n",
    "\n",
    "$$\n",
    "J(\\theta^*) = \\frac{1}{2m} \\sum_{i=1}^m (y^{(i)} - h(x^{(i)}))^2\n",
    "$$\n",
    "\n",
    "Es decir, $J(\\theta^*)$ es el valor mínimo alcanzable de la función de costo.\n",
    "\n",
    "El orden $q$ es un parámetro del modelo, que es el orden del polinomio. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14f587",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b919fe",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "La **regresión lineal** es un método estadístico para modelar la relación entre una variable dependiente $y$ y una o más variables independientes $x$. El objetivo es encontrar la recta (o hiperplano) que mejor se ajusta a los datos.\n",
    "\n",
    "### Modelo matemático (una variable):\n",
    "$$\n",
    "y \\approx h(x) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $y$ es la variable dependiente (lo que queremos predecir).\n",
    "- $x$ es la variable independiente.\n",
    "- $w_0$ es el intercepto (ordenada al origen).\n",
    "- $w_1$ es la pendiente (cuánto cambia $y$ por cada unidad de $x$).\n",
    "\n",
    "### Modelo matemático (múltiples variables):\n",
    "$$\n",
    "y \\approx h(x) = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n\n",
    "$$\n",
    "\n",
    "El objetivo es encontrar los valores de $w_0, w_1, \\ldots, w_n$ que minimizan el error entre las predicciones y los valores reales, normalmente usando el método de mínimos cuadrados:\n",
    "$$\n",
    "\\min_{w} \\sum_{i=1}^m (y^{(i)} - h(x^{(i)}))^2\n",
    "$$\n",
    "\n",
    "Donde $m$ es el número de ejemplos en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7781ea",
   "metadata": {},
   "source": [
    "## Función Logística (Sigmoide)\n",
    "\n",
    "La **función logística** o **sigmoide** es ampliamente utilizada en modelos de clasificación, especialmente en regresión logística y redes neuronales.\n",
    "\n",
    "### Definición matemática\n",
    "\n",
    "La función logística se define como:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "donde:\n",
    "- $z$ es una variable real (puede ser una combinación lineal de entradas y pesos: $z = w_0 + w_1 x_1 + \\cdots + w_n x_n$).\n",
    "\n",
    "### Propiedades\n",
    "- Su salida está acotada entre 0 y 1.\n",
    "- Es una función suave y diferenciable.\n",
    "- Se interpreta como una probabilidad en clasificación binaria.\n",
    "\n",
    "### Paso a paso\n",
    "1. **Calcular $z$:** Sumar el sesgo y las entradas ponderadas.\n",
    "   $$\n",
    "   z = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n\n",
    "   $$\n",
    "2. **Aplicar la función logística:**\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "3. **Interpretar el resultado:**\n",
    "   - Si $\\sigma(z) \\approx 1$, la clase predicha es positiva.\n",
    "   - Si $\\sigma(z) \\approx 0$, la clase predicha es negativa.\n",
    "\n",
    "### Gráfica\n",
    "La función tiene forma de \"S\" y transforma cualquier valor real a un rango entre 0 y 1.\n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc97120",
   "metadata": {},
   "source": [
    "## Clasificación Multiclase: Regresión Logística Multinomial (Softmax)\n",
    "\n",
    "Cuando la variable de salida $y$ puede tomar más de dos valores (clases), el problema es de **clasificación multiclase**. El modelo más común es la **regresión logística multinomial** o **softmax**.\n",
    "\n",
    "### Terminología y notación\n",
    "- $K$: número de clases posibles ($y \\in \\{1, 2, ..., K\\}$).\n",
    "- $x \\in \\mathbb{R}^n$: vector de características de entrada.\n",
    "- $w_k \\in \\mathbb{R}^n$: vector de pesos para la clase $k$.\n",
    "- $b_k$: sesgo para la clase $k$.\n",
    "- $z_k = w_k^T x + b_k$: puntuación (logit) para la clase $k$.\n",
    "\n",
    "### Función softmax\n",
    "La función softmax convierte los logits $z_k$ en probabilidades para cada clase:\n",
    "$$\n",
    "P(y = k | x) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "donde:\n",
    "- $P(y = k | x)$ es la probabilidad de que la entrada $x$ pertenezca a la clase $k$.\n",
    "- $z_k$ es el logit para la clase $k$.\n",
    "\n",
    "### Predicción\n",
    "La clase predicha es aquella con mayor probabilidad:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k P(y = k | x)\n",
    "$$\n",
    "\n",
    "### Función de costo: Entropía cruzada categórica\n",
    "La función de costo para clasificación multiclase es la **entropía cruzada categórica**:\n",
    "$$\n",
    "J(W) = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} \\log \\hat{y}_k^{(i)}\n",
    "$$\n",
    "donde:\n",
    "- $m$ es el número de ejemplos.\n",
    "- $y_k^{(i)}$ es 1 si el ejemplo $i$ pertenece a la clase $k$, 0 en otro caso (codificación one-hot).\n",
    "- $\\hat{y}_k^{(i)}$ es la probabilidad predicha para la clase $k$ en el ejemplo $i$.\n",
    "\n",
    "### Resumen del proceso\n",
    "1. Para cada clase $k$, se calcula $z_k = w_k^T x + b_k$.\n",
    "2. Se aplica la función softmax para obtener probabilidades.\n",
    "3. Se predice la clase con mayor probabilidad.\n",
    "4. Se ajustan los parámetros $W = \\{w_1, ..., w_K\\}$ y $b = \\{b_1, ..., b_K\\}$ minimizando la entropía cruzada categórica.\n",
    "\n",
    "### Ejemplo concreto\n",
    "Supón $K=3$ clases y $x \\in \\mathbb{R}^2$. Para una entrada $x = [1, 2]$ y parámetros:\n",
    "- $w_1 = [1, 0]$, $b_1 = 0$\n",
    "- $w_2 = [0, 1]$, $b_2 = 0$\n",
    "- $w_3 = [-1, -1]$, $b_3 = 0$\n",
    "\n",
    "Calculamos:\n",
    "$z_1 = 1*1 + 0*2 = 1$\n",
    "$z_2 = 0*1 + 1*2 = 2$\n",
    "$z_3 = -1*1 + -1*2 = -3$\n",
    "\n",
    "Softmax:\n",
    "$$\n",
    "P(y=1|x) = \\frac{e^1}{e^1 + e^2 + e^{-3}} \\approx 0.23\n",
    "$$\n",
    "$$\n",
    "P(y=2|x) = \\frac{e^2}{e^1 + e^2 + e^{-3}} \\approx 0.76\n",
    "$$\n",
    "$$\n",
    "P(y=3|x) = \\frac{e^{-3}}{e^1 + e^2 + e^{-3}} \\approx 0.01\n",
    "$$\n",
    "Por lo tanto, la clase predicha es la 2.\n",
    "\n",
    "#### Bibliografía\n",
    "- Bishop, C. M. (2006). Pattern Recognition and Machine Learning.\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b66f09",
   "metadata": {},
   "source": [
    "## Definición formal de función sigmoidal\n",
    "\n",
    "Sea $s: \\mathbb{R} \\to \\mathbb{R}$ una función. Se dice que $s$ es **sigmoidal** si cumple las siguientes propiedades:\n",
    "\n",
    "1. **Acotada:** Existen constantes $a, b \\in \\mathbb{R}$ tales que para todo $x \\in \\mathbb{R}$, $a < s(x) < b$.\n",
    "\n",
    "2. **Clase $C^1$:** $s$ es continuamente diferenciable en todo $\\mathbb{R}$ (es decir, $s \\in C^1(\\mathbb{R})$).\n",
    "\n",
    "3. **Derivada positiva:** Para todo $x \\in \\mathbb{R}$, $s'(x) > 0$.\n",
    "\n",
    "Un ejemplo clásico de función sigmoidal es la función logística:\n",
    "$$\n",
    "s(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Propiedades:\n",
    "\n",
    "Si $s$ es sigmoidal, entonces es monotona y tiene asintotas en $x \\to -\\infty$ y $x \\to +\\infty$.\n",
    "\n",
    "def: La funcion logistica se define como:\n",
    "$$\n",
    "s(x) = \\frac{L}{1 + e^{-\\rho (x - x_0)}}\n",
    "$$      \n",
    "\n",
    "si L = 1, rho = 1, x_0 = 0, entonces:\n",
    "$$\n",
    "s(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35df493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fa73696",
   "metadata": {},
   "source": [
    "## Regresión Logística\n",
    "\n",
    "La **regresión logística** es un modelo estadístico utilizado para problemas de clasificación binaria, es decir, cuando la variable de salida $y$ solo puede tomar dos valores (por ejemplo, 0 o 1, \"no\" o \"sí\").\n",
    "\n",
    "### Motivación\n",
    "A diferencia de la regresión lineal, que predice valores continuos, la regresión logística predice la probabilidad de que una observación pertenezca a una de dos clases.\n",
    "\n",
    "### Modelo matemático\n",
    "La regresión logística modela la probabilidad de la clase positiva ($y=1$) como:\n",
    "$$\n",
    "P(y=1|x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "donde $z = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$ es una combinación lineal de las variables de entrada.\n",
    "\n",
    "La predicción final se obtiene aplicando un umbral (por ejemplo, 0.5):\n",
    "$$\n",
    "\\hat{y} = \\begin{cases}\n",
    "1 & \\text{si } \\sigma(z) \\geq 0.5 \\\\\n",
    "0 & \\text{si } \\sigma(z) < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Función de costo (Log-Loss o Entropía Cruzada)\n",
    "La función de costo utilizada es la **entropía cruzada**:\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
    "$$\n",
    "donde $m$ es el número de ejemplos.\n",
    "\n",
    "### Ejemplo numérico sencillo\n",
    "Supongamos que tenemos un solo predictor $x$ y los siguientes parámetros: $w_0 = -3$, $w_1 = 2$. Queremos predecir la probabilidad de $y=1$ para $x=2$.\n",
    "\n",
    "1. Calculamos $z$:\n",
    "$$\n",
    "z = w_0 + w_1 x = -3 + 2 \\times 2 = 1\n",
    "$$\n",
    "\n",
    "2. Aplicamos la función sigmoide:\n",
    "$$\n",
    "\\sigma(1) = \\frac{1}{1 + e^{-1}} \\approx 0.731\n",
    "$$\n",
    "\n",
    "3. Interpretación:\n",
    "La probabilidad de que $y=1$ dado $x=2$ es aproximadamente 73.1%. Si usamos un umbral de 0.5, predecimos $y=1$.\n",
    "\n",
    "### Generalización a múltiples variables\n",
    "Para $n$ variables de entrada:\n",
    "$$\n",
    "z = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n\n",
    "$$\n",
    "y se aplica la función sigmoide a $z$.\n",
    "\n",
    "### Resumen\n",
    "- La salida de la regresión logística es una probabilidad entre 0 y 1.\n",
    "- Se utiliza para clasificación binaria.\n",
    "- La función de costo es la entropía cruzada.\n",
    "- El modelo se entrena ajustando los pesos $w$ para minimizar la función de costo.\n",
    "\n",
    "#### Bibliografía\n",
    "- Bishop, C. M. (2006). Pattern Recognition and Machine Learning.\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f6b32",
   "metadata": {},
   "source": [
    "### Problema inverso en regresión logística\n",
    "\n",
    "El **problema inverso** en regresión logística consiste en encontrar los parámetros óptimos $w$ (pesos y sesgo) que mejor se ajustan a los datos de entrenamiento. Es decir, dados un conjunto de ejemplos $(x^{(i)}, y^{(i)})$, buscamos los valores de $w$ que minimizan la función de costo.\n",
    "\n",
    "#### Formulación matemática\n",
    "El objetivo es resolver:\n",
    "$$\n",
    "w^* = \\arg\\min_{w} J(w)\n",
    "$$\n",
    "donde $J(w)$ es la función de costo (entropía cruzada):\n",
    "$$\n",
    "J(w) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
    "$$\n",
    "con $\\hat{y}^{(i)} = \\sigma(w_0 + w_1 x_1^{(i)} + ... + w_n x_n^{(i)})$.\n",
    "\n",
    "#### ¿Cómo se resuelve?\n",
    "- Se utilizan algoritmos de optimización numérica, como **gradiente descendente** o variantes (stochastic, mini-batch, etc.), para ajustar los parámetros iterativamente.\n",
    "- En cada iteración, se calcula el gradiente de la función de costo respecto a los parámetros y se actualizan los pesos en la dirección que reduce el error.\n",
    "\n",
    "#### Resumen\n",
    "- El problema inverso es el proceso de entrenamiento: ajustar los parámetros del modelo para que las predicciones se acerquen lo más posible a las etiquetas reales.\n",
    "- El resultado es un conjunto de pesos $w^*$ que minimizan la función de costo sobre los datos de entrenamiento.\n",
    "\n",
    "¿Te gustaría ver un ejemplo de código para el ajuste de parámetros con gradiente descendente?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
