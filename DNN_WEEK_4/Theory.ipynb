{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837fa5bd",
   "metadata": {},
   "source": [
    "# Underfitting an overfitting\n",
    "\n",
    "Gaol: Have a good generalization\n",
    "\n",
    "$$\n",
    "\\hat{y} = h(X; \\theta) = \\text{modelo de red neuronal} \\\\\n",
    "\n",
    "\\text{donde} \\quad \\hat{y} \\text{ es la predicción}, \\quad y \\text{ es el valor real}, \\\\\n",
    "\\text{y buscamos que} \\quad \\hat{y} \\approx y \\quad \\text{para una buena generalización.}\n",
    "$$\n",
    "\n",
    "There are two factors inhere:\n",
    "\n",
    "1) Complexity of the data $D: C_D$\n",
    "2) Complexity of the model: $C_M$\n",
    "\n",
    "La norma $\\ell_2$ (norma euclidiana) de los parámetros se define como:\n",
    "\n",
    "$$\n",
    "\\|\\theta\\|_2^2 = \\sum_{i} \\theta_i^2\n",
    "$$\n",
    "\n",
    "y se utiliza en la regularización L2 (Ridge), añadiendo un término al costo:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\text{Loss} + \\lambda \\|\\theta\\|_2^2\n",
    "$$\n",
    "\n",
    "Esto penaliza grandes valores de los pesos y favorece soluciones más suaves.\n",
    "\n",
    "La norma $\\ell_1$ se define como:\n",
    "\n",
    "$$\n",
    "\\|\\theta\\|_1 = \\sum_{i} |\\theta_i|\n",
    "$$\n",
    "\n",
    "y se usa en regularización L1 (Lasso), añadiendo:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\text{Loss} + \\lambda \\|\\theta\\|_1\n",
    "$$\n",
    "\n",
    "Esto promueve la esparsidad, haciendo que muchos parámetros sean exactamente cero, lo que puede simplificar el modelo y mejorar la interpretabilidad.\n",
    "\n",
    "Ambas normas ayudan a controlar la complejidad del modelo y a evitar el sobreajuste.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
