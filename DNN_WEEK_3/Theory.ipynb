{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55be7407",
   "metadata": {},
   "source": [
    "# Componentes de una Red Neuronal Artificial\n",
    "\n",
    "En el contexto de las redes neuronales artificiales (RNA), cada red está compuesta por varias capas y elementos fundamentales que permiten el procesamiento de información y el aprendizaje automático. A continuación se describen los principales componentes:\n",
    "\n",
    "## Neurona Artificial\n",
    "\n",
    "Una **neurona artificial** es la unidad básica de procesamiento en una red neuronal. Inspirada en la neurona biológica, recibe entradas (inputs), las pondera mediante pesos, suma un sesgo (bias), y aplica una función de activación para producir una salida. Matemáticamente, el funcionamiento de una neurona se expresa como:\n",
    "\n",
    "$$\n",
    "y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $x_i$: entradas,\n",
    "- $w_i$: pesos,\n",
    "- $b$: sesgo,\n",
    "- $f$: función de activación,\n",
    "- $y$: salida.\n",
    "\n",
    "## Capas de una Red Neuronal\n",
    "\n",
    "Las redes neuronales están organizadas en **capas**, cada una compuesta por varias neuronas:\n",
    "\n",
    "### 1. Capa de Entrada (Input Layer)\n",
    "Es la primera capa de la red. Recibe los datos de entrada y los distribuye a la siguiente capa. No realiza procesamiento, solo transmite la información.\n",
    "\n",
    "### 2. Capas Ocultas (Hidden Layers)\n",
    "Son las capas intermedias entre la entrada y la salida. Cada neurona en estas capas procesa la información recibida, permitiendo que la red aprenda representaciones complejas y no lineales. El número y tamaño de las capas ocultas determinan la capacidad de la red para modelar patrones.\n",
    "\n",
    "### 3. Capa de Salida (Output Layer)\n",
    "Es la última capa de la red. Produce la predicción o resultado final del modelo, dependiendo del tipo de problema (clasificación, regresión, etc.).\n",
    "\n",
    "## Otros Componentes Clave\n",
    "\n",
    "- **Pesos (\\(w\\))**: Parámetros que determinan la importancia de cada entrada para una neurona.\n",
    "- **Sesgo (bias, \\(b\\))**: Parámetro adicional que permite ajustar la salida de la neurona independientemente de las entradas.\n",
    "- **Función de Activación (\\(f\\))**: Función matemática que introduce no linealidad, permitiendo que la red aprenda relaciones complejas. Ejemplos: sigmoide, ReLU, tanh.\n",
    "- **Red Neuronal**: Conjunto de neuronas organizadas en capas, conectadas entre sí, que pueden aprender a resolver tareas específicas mediante entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "**Referencias:**  \n",
    "- Goodfellow, Bengio, Courville: Deep Learning (MIT Press, 2016)  \n",
    "- Bishop: Pattern Recognition and Machine Learning (Springer, 2006)  \n",
    "- Haykin: Neural Networks and Learning Machines (Pearson, 2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8393a",
   "metadata": {},
   "source": [
    "\n",
    "## Cálculo Matemático de los $z$ en Cada Capa de una Red Neuronal\n",
    "\n",
    "En una red neuronal multicapa, el cálculo de las activaciones lineales ($z$) para cada neurona de la capa $l$ se realiza mediante la multiplicación de la matriz de pesos de esa capa por el vector de activaciones de la capa anterior, sumando el sesgo correspondiente. Matemáticamente, para la capa $l$:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]} \\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $\\mathbf{z}^{[l]}$: vector de activaciones lineales (pre-activaciones) de la capa $l$.\n",
    "- $\\mathbf{W}^{[l]}$: matriz de pesos de la capa $l$, de dimensión $(n_l, n_{l-1})$.\n",
    "- $\\mathbf{a}^{[l-1]}$: vector de activaciones de la capa anterior ($l-1$), de dimensión $(n_{l-1}, 1)$.\n",
    "- $\\mathbf{b}^{[l]}$: vector de sesgos de la capa $l$, de dimensión $(n_l, 1)$.\n",
    "\n",
    "### Ejemplo de Matriz de Pesos\n",
    "\n",
    "Si la matriz de pesos $\\mathbf{W}^{[l]}$ tiene elementos $w_{ij}^{[l]}$, donde $i$ indica la neurona de la capa actual y $j$ la neurona de la capa anterior, el cálculo para la neurona $i$ de la capa $l$ es:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "z_1^{[l]} \\\\\n",
    "z_2^{[l]} \\\\\n",
    "\\vdots \\\\\n",
    "z_{n_l}^{[l]}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "w_{11}^{[l]} & w_{12}^{[l]} & \\cdots & w_{1 n_{l-1}}^{[l]} \\\\\n",
    "w_{21}^{[l]} & w_{22}^{[l]} & \\cdots & w_{2 n_{l-1}}^{[l]} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n_l 1}^{[l]} & w_{n_l 2}^{[l]} & \\cdots & w_{n_l n_{l-1}}^{[l]}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_1^{[l-1]} \\\\\n",
    "a_2^{[l-1]} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n_{l-1}}^{[l-1]}\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "b_1^{[l]} \\\\\n",
    "b_2^{[l]} \\\\\n",
    "\\vdots \\\\\n",
    "b_{n_l}^{[l]}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_i^{[l]} = \\sum_{j=1}^{n_{l-1}} w_{ij}^{[l]} a_j^{[l-1]} + b_i^{[l]}\n",
    "$$\n",
    "## Cálculo de la Activación $a$ dada una Función de Activación y una Pre-activación $z$\n",
    "\n",
    "Una vez calculado el vector de pre-activaciones $\\mathbf{z}^{[l]}$ en la capa $l$, se obtiene el vector de activaciones $\\mathbf{a}^{[l]}$ aplicando una función de activación $f$ elemento a elemento:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{[l]} = f\\left(\\mathbf{z}^{[l]}\\right)\n",
    "$$\n",
    "\n",
    "Esto significa que para cada neurona $i$ de la capa $l$:\n",
    "\n",
    "$$\n",
    "a_i^{[l]} = f\\left(z_i^{[l]}\\right)\n",
    "$$\n",
    "\n",
    "Las funciones de activación más comunes son:\n",
    "\n",
    "- **Sigmoide:** \n",
    "    $$\n",
    "    f(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "    $$\n",
    "- **Tangente hiperbólica (tanh):**\n",
    "    $$\n",
    "    f(z) = \\tanh(z)\n",
    "    $$\n",
    "- **ReLU (Rectified Linear Unit):**\n",
    "    $$\n",
    "    f(z) = \\max(0, z)\n",
    "    $$\n",
    "\n",
    "La elección de la función de activación depende del problema y de la arquitectura de la red.\n",
    "\n",
    "---\n",
    "\n",
    "**Referencia:**  \n",
    "Goodfellow, Bengio, Courville: Deep Learning (MIT Press, 2016)\n",
    "\n",
    "---\n",
    "\n",
    "**Referencia:**  \n",
    "Goodfellow, Bengio, Courville: Deep Learning (MIT Press, 2016)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1065e7",
   "metadata": {},
   "source": [
    "En general, para una red neuronal con $L$ capas, el número total de parámetros (pesos y sesgos) se calcula como:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{Total de parámetros} = \\sum_{l=1}^{L} \\left( n_l \\cdot n_{l-1} + n_l \\right) \\\\\n",
    "&= \\left( n_1 \\cdot n_0 + n_1 \\right) + \\left( n_2 \\cdot n_1 + n_2 \\right) + \\cdots + \\left( n_L \\cdot n_{L-1} + n_L \\right) \\\\\n",
    "&= \\left( n_1 \\cdot n_0 + n_2 \\cdot n_1 + \\cdots + n_L \\cdot n_{L-1} \\right) + \\left( n_1 + n_2 + \\cdots + n_L \\right) \\\\\n",
    "&= \\sum_{l=1}^{L} n_l \\cdot n_{l-1} + \\sum_{l=1}^{L} n_l\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $n_0$ es el número de neuronas en la capa de entrada,\n",
    "- $n_l$ es el número de neuronas en la capa $l$,\n",
    "- $L$ es el número total de capas (sin contar la de entrada).\n",
    "\n",
    "Cada término $n_l \\cdot n_{l-1}$ corresponde a los pesos entre la capa $l-1$ y la capa $l$, y cada $n_l$ corresponde a los sesgos de la capa $l$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
