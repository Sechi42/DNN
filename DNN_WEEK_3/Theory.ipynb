{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55be7407",
   "metadata": {},
   "source": [
    "# Componentes de una Red Neuronal Artificial\n",
    "\n",
    "En el contexto de las redes neuronales artificiales (RNA), cada red está compuesta por varias capas y elementos fundamentales que permiten el procesamiento de información y el aprendizaje automático. A continuación se describen los principales componentes:\n",
    "\n",
    "## Neurona Artificial\n",
    "\n",
    "Una **neurona artificial** es la unidad básica de procesamiento en una red neuronal. Inspirada en la neurona biológica, recibe entradas (inputs), las pondera mediante pesos, suma un sesgo (bias), y aplica una función de activación para producir una salida. Matemáticamente, el funcionamiento de una neurona se expresa como:\n",
    "\n",
    "$$\n",
    "y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $x_i$: entradas,\n",
    "- $w_i$: pesos,\n",
    "- $b$: sesgo,\n",
    "- $f$: función de activación,\n",
    "- $y$: salida.\n",
    "\n",
    "## Capas de una Red Neuronal\n",
    "\n",
    "Las redes neuronales están organizadas en **capas**, cada una compuesta por varias neuronas:\n",
    "\n",
    "### 1. Capa de Entrada (Input Layer)\n",
    "Es la primera capa de la red. Recibe los datos de entrada y los distribuye a la siguiente capa. No realiza procesamiento, solo transmite la información.\n",
    "\n",
    "### 2. Capas Ocultas (Hidden Layers)\n",
    "Son las capas intermedias entre la entrada y la salida. Cada neurona en estas capas procesa la información recibida, permitiendo que la red aprenda representaciones complejas y no lineales. El número y tamaño de las capas ocultas determinan la capacidad de la red para modelar patrones.\n",
    "\n",
    "### 3. Capa de Salida (Output Layer)\n",
    "Es la última capa de la red. Produce la predicción o resultado final del modelo, dependiendo del tipo de problema (clasificación, regresión, etc.).\n",
    "\n",
    "## Otros Componentes Clave\n",
    "\n",
    "- **Pesos (\\(w\\))**: Parámetros que determinan la importancia de cada entrada para una neurona.\n",
    "- **Sesgo (bias, \\(b\\))**: Parámetro adicional que permite ajustar la salida de la neurona independientemente de las entradas.\n",
    "- **Función de Activación (\\(f\\))**: Función matemática que introduce no linealidad, permitiendo que la red aprenda relaciones complejas. Ejemplos: sigmoide, ReLU, tanh.\n",
    "- **Red Neuronal**: Conjunto de neuronas organizadas en capas, conectadas entre sí, que pueden aprender a resolver tareas específicas mediante entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "**Referencias:**  \n",
    "- Goodfellow, Bengio, Courville: Deep Learning (MIT Press, 2016)  \n",
    "- Bishop: Pattern Recognition and Machine Learning (Springer, 2006)  \n",
    "- Haykin: Neural Networks and Learning Machines (Pearson, 2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8393a",
   "metadata": {},
   "source": [
    "\n",
    "## Cálculo Matemático de los $z$ en Cada Capa de una Red Neuronal\n",
    "\n",
    "En una red neuronal multicapa, el cálculo de las activaciones lineales ($z$) para cada neurona de la capa $l$ se realiza mediante la multiplicación de la matriz de pesos de esa capa por el vector de activaciones de la capa anterior, sumando el sesgo correspondiente. Matemáticamente, para la capa $l$:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]} \\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $\\mathbf{z}^{[l]}$: vector de activaciones lineales (pre-activaciones) de la capa $l$.\n",
    "- $\\mathbf{W}^{[l]}$: matriz de pesos de la capa $l$, de dimensión $(n_l, n_{l-1})$.\n",
    "- $\\mathbf{a}^{[l-1]}$: vector de activaciones de la capa anterior ($l-1$), de dimensión $(n_{l-1}, 1)$.\n",
    "- $\\mathbf{b}^{[l]}$: vector de sesgos de la capa $l$, de dimensión $(n_l, 1)$.\n",
    "\n",
    "### Ejemplo de Matriz de Pesos\n",
    "\n",
    "Si la matriz de pesos $\\mathbf{W}^{[l]}$ tiene elementos $w_{ij}^{[l]}$, donde $i$ indica la neurona de la capa actual y $j$ la neurona de la capa anterior, el cálculo para la neurona $i$ de la capa $l$ es:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "z_1^{[l]} \\\\\n",
    "z_2^{[l]} \\\\\n",
    "\\vdots \\\\\n",
    "z_{n_l}^{[l]}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "w_{11}^{[l]} & w_{12}^{[l]} & \\cdots & w_{1 n_{l-1}}^{[l]} \\\\\n",
    "w_{21}^{[l]} & w_{22}^{[l]} & \\cdots & w_{2 n_{l-1}}^{[l]} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n_l 1}^{[l]} & w_{n_l 2}^{[l]} & \\cdots & w_{n_l n_{l-1}}^{[l]}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_1^{[l-1]} \\\\\n",
    "a_2^{[l-1]} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n_{l-1}}^{[l-1]}\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "b_1^{[l]} \\\\\n",
    "b_2^{[l]} \\\\\n",
    "\\vdots \\\\\n",
    "b_{n_l}^{[l]}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_i^{[l]} = \\sum_{j=1}^{n_{l-1}} w_{ij}^{[l]} a_j^{[l-1]} + b_i^{[l]}\n",
    "$$\n",
    "## Cálculo de la Activación $a$ dada una Función de Activación y una Pre-activación $z$\n",
    "\n",
    "Una vez calculado el vector de pre-activaciones $\\mathbf{z}^{[l]}$ en la capa $l$, se obtiene el vector de activaciones $\\mathbf{a}^{[l]}$ aplicando una función de activación $f$ elemento a elemento:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{[l]} = f\\left(\\mathbf{z}^{[l]}\\right)\n",
    "$$\n",
    "\n",
    "Esto significa que para cada neurona $i$ de la capa $l$:\n",
    "\n",
    "$$\n",
    "a_i^{[l]} = f\\left(z_i^{[l]}\\right)\n",
    "$$\n",
    "\n",
    "Las funciones de activación más comunes son:\n",
    "\n",
    "- **Sigmoide:** \n",
    "    $$\n",
    "    f(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "    $$\n",
    "- **Tangente hiperbólica (tanh):**\n",
    "    $$\n",
    "    f(z) = \\tanh(z)\n",
    "    $$\n",
    "- **ReLU (Rectified Linear Unit):**\n",
    "    $$\n",
    "    f(z) = \\max(0, z)\n",
    "    $$\n",
    "\n",
    "La elección de la función de activación depende del problema y de la arquitectura de la red.\n",
    "\n",
    "---\n",
    "\n",
    "**Referencia:**  \n",
    "Goodfellow, Bengio, Courville: Deep Learning (MIT Press, 2016)\n",
    "\n",
    "---\n",
    "\n",
    "**Referencia:**  \n",
    "Goodfellow, Bengio, Courville: Deep Learning (MIT Press, 2016)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1065e7",
   "metadata": {},
   "source": [
    "En general, para una red neuronal con $L$ capas, el número total de parámetros (pesos y sesgos) se calcula como:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{Total de parámetros} = \\sum_{l=1}^{L} \\left( n_l \\cdot n_{l-1} + n_l \\right) \\\\\n",
    "&= \\left( n_1 \\cdot n_0 + n_1 \\right) + \\left( n_2 \\cdot n_1 + n_2 \\right) + \\cdots + \\left( n_L \\cdot n_{L-1} + n_L \\right) \\\\\n",
    "&= \\left( n_1 \\cdot n_0 + n_2 \\cdot n_1 + \\cdots + n_L \\cdot n_{L-1} \\right) + \\left( n_1 + n_2 + \\cdots + n_L \\right) \\\\\n",
    "&= \\sum_{l=1}^{L} n_l \\cdot n_{l-1} + \\sum_{l=1}^{L} n_l\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $n_0$ es el número de neuronas en la capa de entrada,\n",
    "- $n_l$ es el número de neuronas en la capa $l$,\n",
    "- $L$ es el número total de capas (sin contar la de entrada).\n",
    "\n",
    "Cada término $n_l \\cdot n_{l-1}$ corresponde a los pesos entre la capa $l-1$ y la capa $l$, y cada $n_l$ corresponde a los sesgos de la capa $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7bcca",
   "metadata": {},
   "source": [
    "# Forward Pass Matemático en una Red Neuronal Feedforward\n",
    "\n",
    "## Notación y Definiciones\n",
    "- $L$: número total de capas (sin contar la de entrada)\n",
    "- $n_0, n_1, \\ldots, n_L$: número de neuronas en cada capa\n",
    "- $\\mathbf{a}^{(l)}$: vector de activaciones de la capa $l$, dimensión $(n_l, 1)$\n",
    "- $\\mathbf{z}^{(l)}$: vector de pre-activaciones de la capa $l$, dimensión $(n_l, 1)$\n",
    "- $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$: matriz de pesos de la capa $l$\n",
    "- $\\mathbf{b}^{(l)} \\in \\mathbb{R}^{n_l}$: vector de sesgos de la capa $l$\n",
    "\n",
    "## Ecuaciones Vectorizadas del Forward Pass\n",
    "Para una entrada $\\mathbf{a}^{(0)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)} \\qquad (n_l \\times 1)\n",
    "$$\n",
    "$$\n",
    "\\mathbf{a}^{(l)} = f^{(l)}\\left(\\mathbf{z}^{(l)}\\right) \\qquad (n_l \\times 1)\n",
    "$$\n",
    "\n",
    "Para un batch de $m$ ejemplos ($\\mathbf{A}^{(l-1)}$ de tamaño $n_{l-1} \\times m$):\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{A}^{(l-1)} + \\mathbf{b}^{(l)} \\mathbf{1}_m^T \\qquad (n_l \\times m)\n",
    "$$\n",
    "$$\n",
    "\\mathbf{A}^{(l)} = f^{(l)}\\left(\\mathbf{Z}^{(l)}\\right) \\qquad (n_l \\times m)\n",
    "$$\n",
    "\n",
    "## Ejemplo Numérico Paso a Paso\n",
    "Supongamos una red con:\n",
    "- Entrada: $n_0 = 2$\n",
    "- Capa oculta: $n_1 = 3$\n",
    "- Salida: $n_2 = 2$\n",
    "\n",
    "### Parámetros\n",
    "$$\n",
    "\\mathbf{a}^{(0)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{W}^{(1)} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 2 \\\\ -2 & 1 \\end{pmatrix}, \\quad \\mathbf{b}^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{W}^{(2)} = \\begin{pmatrix} 1 & 0 & -1 \\\\ 2 & -2 & 1 \\end{pmatrix}, \\quad \\mathbf{b}^{(2)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Capa 1 (oculta)\n",
    "$$\n",
    "\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)} =\n",
    "\\begin{pmatrix} 1 & -1 \\\\ 0 & 2 \\\\ -2 & 1 \\end{pmatrix}\n",
    "\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n",
    "$$\n",
    "Calculamos:\n",
    "- Fila 1: $1*1 + (-1)*2 + 0 = 1 - 2 = -1$\n",
    "- Fila 2: $0*1 + 2*2 + 1 = 0 + 4 + 1 = 5$\n",
    "- Fila 3: $-2*1 + 1*2 - 1 = -2 + 2 - 1 = -1$\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(1)} = \\begin{pmatrix} -1 \\\\ 5 \\\\ -1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Activación ReLU\n",
    "$$\n",
    "\\mathbf{a}^{(1)} = \\text{ReLU}(\\mathbf{z}^{(1)}) = \\max(0, \\mathbf{z}^{(1)}) = \\begin{pmatrix} 0 \\\\ 5 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Capa 2 (salida)\n",
    "$$\n",
    "\\mathbf{z}^{(2)} = \\mathbf{W}^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} =\n",
    "\\begin{pmatrix} 1 & 0 & -1 \\\\ 2 & -2 & 1 \\end{pmatrix}\n",
    "\\begin{pmatrix} 0 \\\\ 5 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    "Calculamos:\n",
    "- Fila 1: $1*0 + 0*5 + (-1)*0 + 0 = 0$\n",
    "- Fila 2: $2*0 + (-2)*5 + 1*0 + 1 = 0 - 10 + 1 = -9$\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(2)} = \\begin{pmatrix} 0 \\\\ -9 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Activación Softmax\n",
    "$$\n",
    "\\mathbf{a}^{(2)} = \\text{softmax}(\\mathbf{z}^{(2)}) = \\frac{e^{\\mathbf{z}^{(2)}}}{\\sum_j e^{z_j^{(2)}}}\n",
    "$$\n",
    "Calculamos:\n",
    "- $e^{0} = 1$\n",
    "- $e^{-9} \\approx 0.000123$\n",
    "- Suma: $1 + 0.000123 \\approx 1.000123$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(2)} = \\begin{pmatrix} \\frac{1}{1.000123} \\\\ \\frac{0.000123}{1.000123} \\end{pmatrix} \\approx \\begin{pmatrix} 0.99988 \\\\ 0.00012 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Fórmulas de Activación\n",
    "- **ReLU:** $f(z) = \\max(0, z)$\n",
    "- **Softmax:** $f(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n",
    "\n",
    "## Complejidad Computacional por Capa\n",
    "- Multiplicación de matrices: $O(n_l n_{l-1})$ por ejemplo\n",
    "- Suma de sesgos: $O(n_l)$\n",
    "- Aplicación de activación: $O(n_l)$\n",
    "\n",
    "## Pitfalls Comunes\n",
    "- **Broadcasting:** Asegúrate que $\\mathbf{b}^{(l)}$ se suma correctamente a cada columna del batch.\n",
    "- **Off-by-one shapes:** Verifica dimensiones de $\\mathbf{W}^{(l)}$ y $\\mathbf{a}^{(l-1)}$.\n",
    "- **Softmax numérica:** Para estabilidad, usa $\\text{softmax}(\\mathbf{z}) = \\frac{e^{\\mathbf{z} - \\max(\\mathbf{z})}}{\\sum_j e^{z_j - \\max(\\mathbf{z})}}$\n",
    "\n",
    "---\n",
    "Este ejemplo puede copiarse directamente en un reporte o notebook y se adapta fácilmente a cualquier arquitectura de red neuronal feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3a38f",
   "metadata": {},
   "source": [
    "# Backpropagation en Redes Neuronales Feedforward: Exposición Matemática Avanzada\n",
    " \n",
    "## 1. Estructura de la Red\n",
    "- $L$: número total de capas (sin contar la de entrada)\n",
    "- $n_0, n_1, \\ldots, n_L$: número de neuronas en cada capa\n",
    "- $\\mathbf{a}^{(0)}$: entrada, dimensión $(n_0, 1)$\n",
    "- Para $l = 1, \\ldots, L$:\n",
    "  - $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$: matriz de pesos\n",
    "  - $\\mathbf{b}^{(l)} \\in \\mathbb{R}^{n_l}$: vector de sesgos\n",
    "  - $\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$\n",
    "  - $\\mathbf{a}^{(l)} = \\phi^{(l)}(\\mathbf{z}^{(l)})$\n",
    " \n",
    "---\n",
    " \n",
    "## 2. Función de Pérdida Escalar y Gradientes\n",
    "Sea $\\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y})$ una función de pérdida escalar (ejemplo: cross-entropy para clasificación).\n",
    " \n",
    "El objetivo es calcular los gradientes:\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}}$\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}}$\n",
    " \n",
    "Usando la regla de la cadena, para cada capa $l$:\n",
    " \n",
    "---\n",
    " \n",
    "## 3. Fórmulas Recursivas para el Backward Pass\n",
    "### Error local (delta) en la capa $l$:\n",
    "$$\n",
    "\\delta^{(l)} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(l)}} \\in \\mathbb{R}^{n_l}\n",
    "$$\n",
    " \n",
    "#### Para la capa de salida ($l = L$):\n",
    "- Si $\\phi^{(L)}$ es softmax y $\\mathcal{L}$ es cross-entropy:\n",
    "  $$\n",
    "  \\delta^{(L)} = \\mathbf{a}^{(L)} - \\mathbf{y}\n",
    "  $$\n",
    " \n",
    "#### Para capas ocultas ($l < L$):\n",
    "$$\n",
    "\\delta^{(l)} = \\left( \\mathbf{W}^{(l+1)T} \\delta^{(l+1)} \\right) \\odot \\phi'^{(l)}(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "donde $\\odot$ es el producto elemento a elemento.\n",
    " \n",
    "### Gradientes de pesos y sesgos:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)} \\cdot \\mathbf{a}^{(l-1)T}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}} = \\delta^{(l)}\n",
    "$$\n",
    " \n",
    "---\n",
    " \n",
    "## 4. Ejemplo Numérico Paso a Paso (2 capas: ReLU + Softmax)\n",
    "### Estructura:\n",
    "- Entrada: $n_0 = 2$\n",
    "- Oculta: $n_1 = 3$ (ReLU)\n",
    "- Salida: $n_2 = 2$ (Softmax)\n",
    "- Un solo ejemplo: $\\mathbf{a}^{(0)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$\n",
    " \n",
    "#### Parámetros:\n",
    "$$\n",
    "\\mathbf{W}^{(1)} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 2 \\\\ -2 & 1 \\end{pmatrix}, \\quad \\mathbf{b}^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{W}^{(2)} = \\begin{pmatrix} 1 & 0 & -1 \\\\ 2 & -2 & 1 \\end{pmatrix}, \\quad \\mathbf{b}^{(2)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    " \n",
    "#### Forward Pass:\n",
    "1. Capa oculta:\n",
    "   $$\n",
    "   \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)} = \\begin{pmatrix} -1 \\\\ 5 \\\\ -1 \\end{pmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{a}^{(1)} = \\text{ReLU}(\\mathbf{z}^{(1)}) = \\begin{pmatrix} 0 \\\\ 5 \\\\ 0 \\end{pmatrix}\n",
    "   $$\n",
    " \n",
    "2. Capa salida:\n",
    "   $$\n",
    "   \\mathbf{z}^{(2)} = \\mathbf{W}^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} = \\begin{pmatrix} 0 \\\\ -9 \\end{pmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{a}^{(2)} = \\text{softmax}(\\mathbf{z}^{(2)}) \\approx \\begin{pmatrix} 0.99988 \\\\ 0.00012 \\end{pmatrix}\n",
    "   $$\n",
    " \n",
    "3. Supón $\\mathbf{y} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ (clase 1).\n",
    " \n",
    "#### Backward Pass:\n",
    "1. Capa salida:\n",
    "   $$\n",
    "   \\delta^{(2)} = \\mathbf{a}^{(2)} - \\mathbf{y} = \\begin{pmatrix} -0.00012 \\\\ 0.00012 \\end{pmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(2)}} = \\delta^{(2)} \\cdot \\mathbf{a}^{(1)T}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(2)}} = \\delta^{(2)}\n",
    "   $$\n",
    " \n",
    "2. Capa oculta:\n",
    "   - Derivada de ReLU: $\\phi'^{(1)}(\\mathbf{z}^{(1)}) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$\n",
    "   $$\n",
    "   \\delta^{(1)} = \\left( \\mathbf{W}^{(2)T} \\delta^{(2)} \\right) \\odot \\phi'^{(1)}(\\mathbf{z}^{(1)})\n",
    "   $$\n",
    "   Calcula:\n",
    "   $$\n",
    "   \\mathbf{W}^{(2)T} \\delta^{(2)} = \n",
    "   \\begin{pmatrix}\n",
    "   1 & 2 \\\\\n",
    "   0 & -2 \\\\\n",
    "   -1 & 1\n",
    "   \\end{pmatrix}\n",
    "   \\begin{pmatrix}\n",
    "   -0.00012 \\\\ 0.00012\n",
    "   \\end{pmatrix}\n",
    "   =\n",
    "   \\begin{pmatrix}\n",
    "   1*(-0.00012) + 2*0.00012 \\\\\n",
    "   0*(-0.00012) + (-2)*0.00012 \\\\\n",
    "   -1*(-0.00012) + 1*0.00012\n",
    "   \\end{pmatrix}\n",
    "   =\n",
    "   \\begin{pmatrix}\n",
    "   0.00012 \\\\\n",
    "   -0.00024 \\\\\n",
    "   0.00024\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   \\delta^{(1)} = \\begin{pmatrix} 0.00012 \\\\ -0.00024 \\\\ 0.00024 \\end{pmatrix} \\odot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -0.00024 \\\\ 0 \\end{pmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(1)}} = \\delta^{(1)} \\cdot \\mathbf{a}^{(0)T}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(1)}} = \\delta^{(1)}\n",
    "   $$\n",
    " \n",
    "---\n",
    " \n",
    "## 5. Procesamiento por Batch (Forma Matricial)\n",
    "Para un batch de $m$ ejemplos:\n",
    "- $\\mathbf{A}^{(l-1)} \\in \\mathbb{R}^{n_{l-1} \\times m}$\n",
    "- $\\mathbf{Z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{A}^{(l-1)} + \\mathbf{b}^{(l)} \\mathbf{1}_m^T$\n",
    "- $\\mathbf{A}^{(l)} = \\phi^{(l)}(\\mathbf{Z}^{(l)})$\n",
    " \n",
    "En backpropagation:\n",
    "- $\\Delta^{(l)} \\in \\mathbb{R}^{n_l \\times m}$: matriz de errores para el batch\n",
    "- Gradientes:\n",
    "  $$\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\Delta^{(l)} \\cdot \\mathbf{A}^{(l-1)T}\n",
    "  $$\n",
    "  $$\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(l)}} = \\Delta^{(l)} \\cdot \\mathbf{1}_m\n",
    "  $$\n",
    " \n",
    "---\n",
    " \n",
    "## 6. Complejidad Computacional y Problemas de Gradiente\n",
    "- **Multiplicación de matrices:** $O(n_l n_{l-1} m)$ por capa y batch\n",
    "- **Suma de sesgos:** $O(n_l m)$\n",
    "- **Aplicación de activación:** $O(n_l m)$\n",
    " \n",
    "**Problemas:**\n",
    "- **Gradiente que desaparece:** En redes profundas, derivadas pequeñas pueden hacer que los gradientes se vuelvan insignificantes, dificultando el aprendizaje.\n",
    "- **Gradiente que explota:** Derivadas grandes pueden hacer que los gradientes crezcan exponencialmente, causando inestabilidad numérica.\n",
    "- **Soluciones:** Inicialización adecuada, normalización, funciones de activación como ReLU, regularización.\n",
    " \n",
    "---\n",
    " \n",
    "**Este desarrollo es adecuado para reportes académicos y tesis, y puede copiarse directamente en un notebook Jupyter con formato LaTeX.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc3545",
   "metadata": {},
   "source": [
    "# Funciones de Activación Populares en Redes Neuronales\n",
    " \n",
    "A continuación se describen las funciones de activación más utilizadas, sus fórmulas matemáticas, ventajas y desventajas.\n",
    " \n",
    "## 1. Sigmoide ($\\sigma$)\n",
    "**Fórmula:**\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "**Pros:**\n",
    "- Salida en rango $(0, 1)$, útil para probabilidades.\n",
    "- Suave y diferenciable.\n",
    "**Contras:**\n",
    "- Gradiente se desvanece para valores extremos (problema de vanishing gradient).\n",
    "- Salidas no centradas en cero.\n",
    "- Saturación para $|z|$ grande.\n",
    " \n",
    "## 2. Tangente Hiperbólica (tanh)\n",
    "**Fórmula:**\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "$$\n",
    "**Pros:**\n",
    "- Salida en rango $(-1, 1)$, centrada en cero.\n",
    "- Suave y diferenciable.\n",
    "**Contras:**\n",
    "- Gradiente se desvanece para valores extremos.\n",
    "- Saturación para $|z|$ grande.\n",
    " \n",
    "## 3. ReLU (Rectified Linear Unit)\n",
    "**Fórmula:**\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "**Pros:**\n",
    "- Computacionalmente eficiente.\n",
    "- Mitiga el problema de vanishing gradient para $z > 0$.\n",
    "- Favorece la esparsidad.\n",
    "**Contras:**\n",
    "- \"Neuronas muertas\": gradiente cero para $z < 0$.\n",
    "- No acotada superiormente.\n",
    " \n",
    "## 4. Leaky ReLU\n",
    "**Fórmula:**\n",
    "$$\n",
    "\\text{Leaky ReLU}(z) = \\begin{cases} z & \\text{si } z > 0 \\\\ \\alpha z & \\text{si } z \\leq 0 \\end{cases}\n",
    "$$\n",
    "donde $\\alpha$ es pequeño (ej. $0.01$).\n",
    "**Pros:**\n",
    "- Evita neuronas muertas.\n",
    "- Mantiene eficiencia de ReLU.\n",
    "**Contras:**\n",
    "- Introduce un pequeño sesgo negativo.\n",
    " \n",
    "## 5. Softmax\n",
    "**Fórmula:**\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "**Pros:**\n",
    "- Convierte vectores en distribuciones de probabilidad.\n",
    "- Útil para clasificación multiclase.\n",
    "**Contras:**\n",
    "- No se usa en capas ocultas.\n",
    "- Sensible a valores extremos (puede causar saturación numérica).\n",
    " \n",
    "## 6. ELU (Exponential Linear Unit)\n",
    "**Fórmula:**\n",
    "$$\n",
    "\\text{ELU}(z) = \\begin{cases} z & \\text{si } z > 0 \\\\ \\alpha (e^{z} - 1) & \\text{si } z \\leq 0 \\end{cases}\n",
    "$$\n",
    "**Pros:**\n",
    "- Salidas negativas ayudan a centrar activaciones.\n",
    "- Mitiga el problema de neuronas muertas.\n",
    "**Contras:**\n",
    "- Más costosa computacionalmente que ReLU.\n",
    " \n",
    "## 7. Swish\n",
    "**Fórmula:**\n",
    "$$\n",
    "\\text{Swish}(z) = z \\cdot \\sigma(z)\n",
    "$$\n",
    "**Pros:**\n",
    "- Suave, no monótona.\n",
    "- Mejora el rendimiento en redes profundas.\n",
    "**Contras:**\n",
    "- Más costosa computacionalmente.\n",
    " \n",
    "---\n",
    "**Resumen:** La elección de la función de activación depende del problema, arquitectura y profundidad de la red. ReLU y variantes dominan en redes profundas, mientras que sigmoide y softmax se usan en salidas para tareas de clasificación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
